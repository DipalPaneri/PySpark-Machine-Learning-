{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "552fe0ba-9d95-4b3e-9065-604173a72cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74aad948-d62f-45f3-87dd-28e49c5e5f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Titanic Logistic Regression Classification\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3471bbee-8e35-4b17-98c0-907595ca7a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"titanic.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d611803c-3809-4ea7-ae5c-e8710bcadaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2dca3ea-37c6-45ec-acbe-9fe2a51efb61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PassengerId',\n",
       " 'Survived',\n",
       " 'Pclass',\n",
       " 'Name',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'SibSp',\n",
       " 'Parch',\n",
       " 'Ticket',\n",
       " 'Fare',\n",
       " 'Cabin',\n",
       " 'Embarked']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e63dafa2-c8e7-4a84-9328-5ae72b9b7f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_cols = df.select(['Survived','Pclass','Sex','Age','SibSp',\n",
    "                     'Parch','Fare','Embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e574f2e-b122-4dca-854e-9d49ee42e32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handling missing values and dropiing it for this project. \n",
    "my_final_data = my_cols.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47ff86f7-680a-4941-9f85-a1ae87c12d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+----+-----+-----+-------+--------+\n",
      "|Survived|Pclass|   Sex| Age|SibSp|Parch|   Fare|Embarked|\n",
      "+--------+------+------+----+-----+-----+-------+--------+\n",
      "|       0|     3|  male|22.0|    1|    0|   7.25|       S|\n",
      "|       1|     1|female|38.0|    1|    0|71.2833|       C|\n",
      "|       1|     3|female|26.0|    0|    0|  7.925|       S|\n",
      "|       1|     1|female|35.0|    1|    0|   53.1|       S|\n",
      "|       0|     3|  male|35.0|    0|    0|   8.05|       S|\n",
      "|       0|     1|  male|54.0|    0|    0|51.8625|       S|\n",
      "|       0|     3|  male| 2.0|    3|    1| 21.075|       S|\n",
      "|       1|     3|female|27.0|    0|    2|11.1333|       S|\n",
      "|       1|     2|female|14.0|    1|    0|30.0708|       C|\n",
      "|       1|     3|female| 4.0|    1|    1|   16.7|       S|\n",
      "|       1|     1|female|58.0|    0|    0|  26.55|       S|\n",
      "|       0|     3|  male|20.0|    0|    0|   8.05|       S|\n",
      "|       0|     3|  male|39.0|    1|    5| 31.275|       S|\n",
      "|       0|     3|female|14.0|    0|    0| 7.8542|       S|\n",
      "|       1|     2|female|55.0|    0|    0|   16.0|       S|\n",
      "|       0|     3|  male| 2.0|    4|    1| 29.125|       Q|\n",
      "|       0|     3|female|31.0|    1|    0|   18.0|       S|\n",
      "|       0|     2|  male|35.0|    0|    0|   26.0|       S|\n",
      "|       1|     2|  male|34.0|    0|    0|   13.0|       S|\n",
      "|       1|     3|female|15.0|    0|    0| 8.0292|       Q|\n",
      "+--------+------+------+----+-----+-----+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_final_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efe571a-8aa5-4ec5-a793-09413dcdddbe",
   "metadata": {},
   "source": [
    "##Working with Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bb08d8f-2953-49ee-8d78-adafaf8448a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer, OneHotEncoder, StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3ab2b53-e4fc-4454-8a8f-7766a7d45fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using StringIndexer we convert every string into a number and then one hot encode on the stringIndexer object.\n",
    "gender_indexer = StringIndexer(inputCol='Sex', outputCol='SexIndexed')\n",
    "gender_encoder = OneHotEncoder(inputCol='SexIndexed', outputCol='SexVect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cafcd216-e8e4-4142-9ae9-aa0830bfabab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets do the same for embarked column\n",
    "embarked_indexer= StringIndexer(inputCol='Embarked', outputCol='EmbarkedIndex')\n",
    "embarked_encoder= OneHotEncoder(inputCol='EmbarkedIndex', outputCol='EmbarkedVect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e6be1711-aaa7-469d-97f5-e17c8db3bebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['Pclass','SexVect','EmbarkedVect','Age','SibSp','Parch','Fare'],\n",
    "                           outputCol='feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0496e78-25a9-4d8b-ba0c-4a8818d5f1e6",
   "metadata": {},
   "source": [
    "Now Lets Build Pipeline and Call The String indexer and One hot Encoder that we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a2d69fe4-9cf1-41d1-bb37-2b0ad48b4d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "291f8223-ceea-44da-8229-6da563cbeabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0740c0e-c277-4e05-8528-2069bd4b1f24",
   "metadata": {},
   "source": [
    "pipeline sets different stages for a very complex process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a45716d5-eb4a-40cd-a1d6-4e8884ab0573",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets create out Logistic regression object\n",
    "logreg_titanic = LogisticRegression(featuresCol='feature', labelCol='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "af0d5967-8229-4f8d-840e-7ba8fad8d11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[gender_indexer, embarked_indexer, gender_encoder, embarked_encoder, assembler, logreg_titanic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1505dc38-bb0c-4e68-9eff-4bafd3b40220",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = my_final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "682973e5-184a-48ce-8061-97965616242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c8994db-b0f9-4e81-b02b-ec356db85a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|Survived|\n",
      "+--------+\n",
      "|       0|\n",
      "|       1|\n",
      "|       1|\n",
      "|       1|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       1|\n",
      "|       1|\n",
      "|       1|\n",
      "|       1|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       1|\n",
      "|       0|\n",
      "|       1|\n",
      "|       0|\n",
      "|       1|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#due to column doent exist in dataframe error I made a few changes to my_final_data dataFrame\n",
    "if 'Survived' not in my_final_data.columns:\n",
    "    print(\"Survived column does not exist in the DataFrame\")\n",
    "else:\n",
    "    # Perform operations with 'Survived' column\n",
    "    # Example: Select the 'Survived' column\n",
    "    df.select('Survived').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9a91dee4-966b-4774-bfdc-495ced72ef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time to transform out test data\n",
    "results = fit_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68951bf4-c327-4bca-a138-1e84890166ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets Evaluate the train data with test data\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1efb5c35-6db9-401e-9032-493e7caa6f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8ae8df31-c0ca-4cc5-82f8-f05598ff2926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|Survived|prediction|\n",
      "+--------+----------+\n",
      "|       0|       1.0|\n",
      "|       0|       1.0|\n",
      "|       0|       1.0|\n",
      "|       0|       1.0|\n",
      "|       0|       1.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "|       0|       0.0|\n",
      "+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.select('Survived','prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a828242f-cb52-47fe-a61d-000918a4c6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets look at Area Under the Curve\n",
    "AUC = my_eval.evaluate(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb108417-77cb-4816-ba2c-372f30b0396b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7480650154798762"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets look at the AUC\n",
    "AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b423026c-1c8e-4595-a392-62a92a0fc73e",
   "metadata": {},
   "source": [
    "AUC results shows the curve on side of 1 which means the model prediction was 74% matches. \n",
    "AUC = Area Under the Curve.\r\n",
    "AUROC = Area Under the Receiver Operating Characteristic curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8733ff3-ffd3-4de8-b344-c20c8503b9a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
